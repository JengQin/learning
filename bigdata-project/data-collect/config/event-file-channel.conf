a1.sources=s1
a1.channels=c1
a1.sinks=sk1

## source
a1.sources.s1.type=TAILDIR
## position信息存放的路径
a1.sources.s1.positionFile=/work/flume-collect/position/taildirSource_position.json
## position信息的输出时间间隔ms
a1.sources.s1.filegroups=f1
a1.sources.s1.filegroups.f1=/work/data-producer/event_logs/.*log

## channel
a1.channels.c1.type=FILE
a1.channels.c1.checkpointDir=/work/flume-collect/checkpoint/tail_dir
a1.channels.c1.dataDirs=/work/flume-collect/checkpoint/tail_dir_data
## channel支持最大的事务数量
a1.channels.c1.transactionCapacity=10000
## channel的最大容量
a1.channels.c1.capacity=1000000
## checkpoint的触发间隔ms
a1.channels.c1.checkpointInterval=30000
##在channel通道被close时，是否进行checkpoint，进行checkpoint可以避免重启后source重新put事务到channel
a1.channels.c1.checkpointOnClose=true

## sink
a1.sinks.sk1.type=org.apache.flume.sink.kafka.KafkaSink
a1.sinks.sk1.kafka.topic=src_event_all
a1.sinks.sk1.kafka.bootstrap.servers=master1:9092,core1:9092,core2:9092
# sink到kafka的批容量，较大的batch可以增加吞吐量，但同时也会增加延迟
a1.sinks.sk1.kafka.flumeBatchSize=10
# kafka producer停留多久就send一次数据
a1.sinks.sk1.kafka.producer.linger.ms=5
a1.sinks.sk1.kafka.producer.acks=-1
a1.sinks.sk1.kafka.producer.retries=10
a1.sinks.sk1.kafka.producer.compression.type=snappy

a1.sources.s1.channels=c1
a1.sinks.sk1.channel=c1
